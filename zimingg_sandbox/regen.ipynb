{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'PIL'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# from regen_pipe import ReSDPipeline\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[1;32m      6\u001b[0m DEVICE \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDevice is \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m DEVICE)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'PIL'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "# from regen_pipe import ReSDPipeline\n",
    "from PIL import Image\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"Device is \" + DEVICE)\n",
    "if DEVICE != 'cuda':\n",
    "    print(\"ERROR: This attack pipeline only works on CUDA. Please stop and find a CUDA enabled GPU instance.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Credit: https://github.com/XuandongZhao/WatermarkAttacker/blob/main/regen_pipe.py\n",
    "\"\"\"\n",
    "\n",
    "from typing import Callable, List, Optional, Union\n",
    "import torch\n",
    "from diffusers import StableDiffusionPipeline\n",
    "from diffusers.pipelines.stable_diffusion import StableDiffusionPipelineOutput\n",
    "\n",
    "class ReSDPipeline(StableDiffusionPipeline):\n",
    "    @torch.no_grad()\n",
    "    def __call__(\n",
    "            self,\n",
    "            prompt: Union[str, List[str]],\n",
    "            prompt1_steps: Optional[int] = None,\n",
    "            prompt2: Optional[str] = None,\n",
    "            head_start_latents: Optional[Union[torch.FloatTensor, list]] = None,\n",
    "            head_start_step: Optional[int] = None,\n",
    "            height: Optional[int] = None,\n",
    "            width: Optional[int] = None,\n",
    "            num_inference_steps: int = 50,\n",
    "            guidance_scale: float = 7.5,\n",
    "            negative_prompt: Optional[Union[str, List[str]]] = None,\n",
    "            num_images_per_prompt: Optional[int] = 1,\n",
    "            eta: float = 0.0,\n",
    "            generator: Optional[torch.Generator] = None,\n",
    "            latents: Optional[torch.FloatTensor] = None,\n",
    "            output_type: Optional[str] = \"pil\",\n",
    "            return_dict: bool = True,\n",
    "            callback: Optional[Callable[[int, int, torch.FloatTensor], None]] = None,\n",
    "            callback_steps: Optional[int] = 1,\n",
    "    ):\n",
    "        r\"\"\"\n",
    "        Function invoked when calling the pipeline for generation.\n",
    "\n",
    "        Args:\n",
    "            prompt (`str` or `List[str]`):\n",
    "                The prompt or prompts to guide the image generation.\n",
    "            height (`int`, *optional*, defaults to self.unet.config.sample_size * self.vae_scale_factor):\n",
    "                The height in pixels of the generated image.\n",
    "            width (`int`, *optional*, defaults to self.unet.config.sample_size * self.vae_scale_factor):\n",
    "                The width in pixels of the generated image.\n",
    "            num_inference_steps (`int`, *optional*, defaults to 50):\n",
    "                The number of denoising steps. More denoising steps usually lead to a higher quality image at the\n",
    "                expense of slower inference.\n",
    "            guidance_scale (`float`, *optional*, defaults to 7.5):\n",
    "                Guidance scale as defined in [Classifier-Free Diffusion Guidance](https://arxiv.org/abs/2207.12598).\n",
    "                `guidance_scale` is defined as `w` of equation 2. of [Imagen\n",
    "                Paper](https://arxiv.org/pdf/2205.11487.pdf). Guidance scale is enabled by setting `guidance_scale >\n",
    "                1`. Higher guidance scale encourages to generate images that are closely linked to the text `prompt`,\n",
    "                usually at the expense of lower image quality.\n",
    "            negative_prompt (`str` or `List[str]`, *optional*):\n",
    "                The prompt or prompts not to guide the image generation. Ignored when not using guidance (i.e., ignored\n",
    "                if `guidance_scale` is less than `1`).\n",
    "            num_images_per_prompt (`int`, *optional*, defaults to 1):\n",
    "                The number of images to generate per prompt.\n",
    "            eta (`float`, *optional*, defaults to 0.0):\n",
    "                Corresponds to parameter eta (Î·) in the DDIM paper: https://arxiv.org/abs/2010.02502. Only applies to\n",
    "                [`schedulers.DDIMScheduler`], will be ignored for others.\n",
    "            generator (`torch.Generator`, *optional*):\n",
    "                A [torch generator](https://pytorch.org/docs/stable/generated/torch.Generator.html) to make generation\n",
    "                deterministic.\n",
    "            latents (`torch.FloatTensor`, *optional*):\n",
    "                Pre-generated noisy latents, sampled from a Gaussian distribution, to be used as inputs for image\n",
    "                generation. Can be used to tweak the same generation with different prompts. If not provided, a latents\n",
    "                tensor will ge generated by sampling using the supplied random `generator`.\n",
    "            output_type (`str`, *optional*, defaults to `\"pil\"`):\n",
    "                The output format of the generate image. Choose between\n",
    "                [PIL](https://pillow.readthedocs.io/en/stable/): `PIL.Image.Image` or `np.array`.\n",
    "            return_dict (`bool`, *optional*, defaults to `True`):\n",
    "                Whether or not to return a [`~pipelines.stable_diffusion.StableDiffusionPipelineOutput`] instead of a\n",
    "                plain tuple.\n",
    "            callback (`Callable`, *optional*):\n",
    "                A function that will be called every `callback_steps` steps during inference. The function will be\n",
    "                called with the following arguments: `callback(step: int, timestep: int, latents: torch.FloatTensor)`.\n",
    "            callback_steps (`int`, *optional*, defaults to 1):\n",
    "                The frequency at which the `callback` function will be called. If not specified, the callback will be\n",
    "                called at every step.\n",
    "\n",
    "        Returns:\n",
    "            [`~pipelines.stable_diffusion.StableDiffusionPipelineOutput`] or `tuple`:\n",
    "            [`~pipelines.stable_diffusion.StableDiffusionPipelineOutput`] if `return_dict` is True, otherwise a `tuple.\n",
    "            When returning a tuple, the first element is a list with the generated images, and the second element is a\n",
    "            list of `bool`s denoting whether the corresponding generated image likely represents \"not-safe-for-work\"\n",
    "            (nsfw) content, according to the `safety_checker`.\n",
    "        \"\"\"\n",
    "        # import pdb; pdb.set_trace()\n",
    "        # 0. Default height and width to unet\n",
    "        height = height or self.unet.config.sample_size * self.vae_scale_factor\n",
    "        width = width or self.unet.config.sample_size * self.vae_scale_factor\n",
    "\n",
    "        # 1. Check inputs. Raise error if not correct\n",
    "        self.check_inputs(prompt, height, width, callback_steps)\n",
    "\n",
    "        # 2. Define call parameters\n",
    "        batch_size = 1 if isinstance(prompt, str) else len(prompt)\n",
    "        device = self._execution_device\n",
    "        # here `guidance_scale` is defined analog to the guidance weight `w` of equation (2)\n",
    "        # of the Imagen paper: https://arxiv.org/pdf/2205.11487.pdf . `guidance_scale = 1`\n",
    "        # corresponds to doing no classifier free guidance.\n",
    "        do_classifier_free_guidance = guidance_scale > 1.0\n",
    "\n",
    "        # 3. Encode input prompt\n",
    "        text_embeddings = self._encode_prompt(\n",
    "            prompt, device, num_images_per_prompt, do_classifier_free_guidance, negative_prompt\n",
    "        )\n",
    "\n",
    "        if prompt2 is not None:\n",
    "            text_embeddings2 = self._encode_prompt(\n",
    "                prompt2, device, num_images_per_prompt, do_classifier_free_guidance, negative_prompt\n",
    "            )\n",
    "\n",
    "        # 4. Prepare timesteps\n",
    "        self.scheduler.set_timesteps(num_inference_steps, device=device)\n",
    "        timesteps = self.scheduler.timesteps\n",
    "        # print(timesteps)\n",
    "\n",
    "        # 5. Prepare latent variables\n",
    "        if head_start_latents is None:\n",
    "            num_channels_latents = self.unet.in_channels\n",
    "            latents = self.prepare_latents(\n",
    "                batch_size * num_images_per_prompt,\n",
    "                num_channels_latents,\n",
    "                height,\n",
    "                width,\n",
    "                text_embeddings.dtype,\n",
    "                device,\n",
    "                generator,\n",
    "                latents,\n",
    "            )\n",
    "        else:\n",
    "            if type(head_start_latents) == list:\n",
    "                latents = head_start_latents[-1]\n",
    "                assert len(head_start_latents) == self.scheduler.config.solver_order\n",
    "\n",
    "            else:\n",
    "                latents = head_start_latents  # if there is a head start\n",
    "        # 6. Prepare extra step kwargs. TODO: Logic should ideally just be moved out of the pipeline\n",
    "        extra_step_kwargs = self.prepare_extra_step_kwargs(generator, eta)\n",
    "\n",
    "        # 7. Denoising loop\n",
    "        num_warmup_steps = len(timesteps) - num_inference_steps * self.scheduler.order\n",
    "        with self.progress_bar(total=num_inference_steps) as progress_bar:\n",
    "            for i, t in enumerate(timesteps):\n",
    "                # print((i, t))\n",
    "                if not head_start_step or i >= head_start_step:  # if there is no head start or we reached the hs step\n",
    "                    # expand the latents if we are doing classifier free guidance\n",
    "                    # print(latents.shape)\n",
    "                    latent_model_input = torch.cat([latents] * 2) if do_classifier_free_guidance else latents\n",
    "                    latent_model_input = self.scheduler.scale_model_input(latent_model_input, t)\n",
    "\n",
    "                    # predict the noise residual\n",
    "                    if prompt1_steps is None or i < prompt1_steps:\n",
    "                        noise_pred = self.unet(latent_model_input, t, encoder_hidden_states=text_embeddings).sample\n",
    "                    else:\n",
    "                        # print(f'i = {i}, atteding to prompt2')\n",
    "                        noise_pred = self.unet(latent_model_input, t, encoder_hidden_states=text_embeddings2).sample\n",
    "\n",
    "                    # perform guidance\n",
    "                    if do_classifier_free_guidance:\n",
    "                        noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
    "                        noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
    "\n",
    "                        # compute the previous noisy sample x_t -> x_t-1\n",
    "                        latents = self.scheduler.step(noise_pred, t, latents, **extra_step_kwargs).prev_sample\n",
    "\n",
    "                # call the callback, if provided\n",
    "                if (i + 1) > num_warmup_steps and (i + 1) % self.scheduler.order == 0:\n",
    "                    progress_bar.update()\n",
    "                    if callback is not None and i % callback_steps == 0:\n",
    "                        callback(i, t, latents)\n",
    "\n",
    "        # 8. Post-processing\n",
    "        image = self.decode_latents(latents)\n",
    "\n",
    "        # 9. Run safety checker\n",
    "        has_nsfw_concept = False\n",
    "        # image, has_nsfw_concept = self.run_safety_checker(image, device, text_embeddings.dtype)\n",
    "\n",
    "        # 10. Convert to PIL\n",
    "        if output_type == \"pil\":\n",
    "            image = self.numpy_to_pil(image)\n",
    "\n",
    "        if not return_dict:\n",
    "            return (image, has_nsfw_concept)\n",
    "\n",
    "        return StableDiffusionPipelineOutput(images=image, nsfw_content_detected=has_nsfw_concept)\n",
    "\n",
    "#TODO cite this too\n",
    "class DiffWMAttacker:\n",
    "    def __init__(self, pipe, noise_step=60, captions={}):\n",
    "        self.pipe = pipe\n",
    "        self.noise_step = noise_step\n",
    "        self.captions = captions\n",
    "        print(\n",
    "            f\"Diffuse attack initialized with noise step {self.noise_step} and use prompt {len(self.captions)}\"\n",
    "        )\n",
    "\n",
    "    def attack(self, image, return_latents=False, return_dist=False):\n",
    "        with torch.no_grad():\n",
    "            generator = torch.Generator(DEVICE).manual_seed(1024)\n",
    "            latents_buf = []\n",
    "            prompts_buf = []\n",
    "            outs_buf = []\n",
    "            timestep = torch.tensor(\n",
    "                [self.noise_step], dtype=torch.long, device=DEVICE\n",
    "            )\n",
    "            ret_latents = []\n",
    "\n",
    "            def batched_attack(latents_buf, prompts_buf, outs_buf):\n",
    "                latents = torch.cat(latents_buf, dim=0)\n",
    "                images = self.pipe(\n",
    "                    prompts_buf,\n",
    "                    head_start_latents=latents,\n",
    "                    head_start_step=50 - max(self.noise_step // 20, 1),\n",
    "                    guidance_scale=7.5,\n",
    "                    generator=generator,\n",
    "                )\n",
    "                images = images[0]\n",
    "                for img, out in zip(images, outs_buf):\n",
    "                    return img\n",
    "\n",
    "            img = np.asarray(image) / 255\n",
    "            img = (img - 0.5) * 2\n",
    "            img = torch.tensor(img).permute(2, 0, 1).unsqueeze(0)\n",
    "            latents = self.pipe.vae.encode(\n",
    "                img.to(device=DEVICE, dtype=torch.float16) #remove when running on cuda?\n",
    "            ).latent_dist\n",
    "            latents = latents.sample(generator) * self.pipe.vae.config.scaling_factor\n",
    "            noise = torch.randn(\n",
    "                [1, 4, img.shape[-2] // 8, img.shape[-1] // 8], device=DEVICE\n",
    "            )\n",
    "\n",
    "            latents = self.pipe.scheduler.add_noise(latents, noise, timestep).type(\n",
    "                torch.half\n",
    "            )\n",
    "            latents_buf.append(latents)\n",
    "            outs_buf.append(\"\")\n",
    "            prompts_buf.append(\"\")\n",
    "\n",
    "            img = batched_attack(latents_buf, prompts_buf, outs_buf)\n",
    "            return img\n",
    "\n",
    "\n",
    "def remove_watermark(attack_method, image, strength, model):\n",
    "    # create attacker\n",
    "    print(f\"Creating attacker {attack_method}...\")\n",
    "    if attack_method == \"regen_diffusion\":\n",
    "        pipe = ReSDPipeline.from_pretrained(\n",
    "            model, torch_dtype=torch.float16, revision=\"fp16\"\n",
    "        )\n",
    "        # pipe.set_progress_bar_config(disable=True)\n",
    "        pipe.to(DEVICE)\n",
    "        attacker = DiffWMAttacker(pipe, noise_step=strength, captions={})\n",
    "\n",
    "    else:\n",
    "        raise Exception(f\"Unknown attacking method: {attack_method}!\")\n",
    "\n",
    "    img = attacker.attack(image)\n",
    "    return img\n",
    "\n",
    "def rinse_nxDiff(image, strength=60, n=4, model=\"stabilityai/stable-diffusion-2-1\"):\n",
    "    # first_attack = True\n",
    "    attack = \"regen_diffusion\"\n",
    "    for i in range(n):\n",
    "        # if first_attack:\n",
    "        #     image = remove_watermark(attack, image, strength, model)\n",
    "        #     first_attack = False\n",
    "        # else:\n",
    "        image = remove_watermark(attack, image, strength, model)\n",
    "    return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving to ./attacked_strength20_n2_1-4.png\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'img_attacked' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m out_fname \u001b[38;5;241m=\u001b[39m get_output_filename(output_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./\u001b[39m\u001b[38;5;124m\"\u001b[39m, strength\u001b[38;5;241m=\u001b[39mstrength, n\u001b[38;5;241m=\u001b[39mn_rinse, notes\u001b[38;5;241m=\u001b[39mmodel[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m3\u001b[39m:])\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSaving to \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m out_fname)\n\u001b[0;32m---> 19\u001b[0m \u001b[43mimg_attacked\u001b[49m\u001b[38;5;241m.\u001b[39msave(out_fname)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'img_attacked' is not defined"
     ]
    }
   ],
   "source": [
    "# Run attack on an image, for example\n",
    "fname_in = \"./watermarked.png\"\n",
    "watermarked = Image.open(fname_in)\n",
    "\n",
    "def get_output_filename(output_path:str, strength:int, n:int, notes:str):\n",
    "    return output_path + \"attacked_strength\" + str(strength) + \"_n\" + str(n) + \"_\" + notes + \".png\"\n",
    "\n",
    "# Model selection: [\"CompVis/stable-diffusion-v1-4\",\"stabilityai/stable-diffusion-2-1\"]\n",
    "paper_rinse2xdiff_config = (\"CompVis/stable-diffusion-v1-4\", 2, 20)\n",
    "paper_rinse4xdiff_config = (\"CompVis/stable-diffusion-v1-4\", 4, 10)\n",
    "dj_rinse1xdiff_config = (\"stabilityai/stable-diffusion-2-1\", 1, 10)\n",
    "\n",
    "config = paper_rinse2xdiff_config\n",
    "\n",
    "model, n_rinse, strength = config\n",
    "img_attacked = rinse_nxDiff(watermarked, strength=strength, n=n_rinse, model=model)\n",
    "out_fname = get_output_filename(output_path=\"./\", strength=strength, n=n_rinse, notes=model[-3:])\n",
    "print(\"Saving to \" + out_fname)\n",
    "img_attacked.save(out_fname)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "idlf24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
