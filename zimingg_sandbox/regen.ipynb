{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device is cuda\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"Device is \" + DEVICE)\n",
    "if DEVICE != 'cuda':\n",
    "    print(\"ERROR: This attack pipeline only works on CUDA. Please stop and find a CUDA enabled GPU instance.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Credit: https://github.com/XuandongZhao/WatermarkAttacker/blob/main/regen_pipe.py\n",
    "\"\"\"\n",
    "\n",
    "from typing import Callable, List, Optional, Union\n",
    "import torch\n",
    "from diffusers import StableDiffusionPipeline\n",
    "from diffusers.pipelines.stable_diffusion import StableDiffusionPipelineOutput\n",
    "\n",
    "class ReSDPipeline(StableDiffusionPipeline):\n",
    "    @torch.no_grad()\n",
    "    def __call__(\n",
    "            self,\n",
    "            prompt: Union[str, List[str]],\n",
    "            prompt1_steps: Optional[int] = None,\n",
    "            prompt2: Optional[str] = None,\n",
    "            head_start_latents: Optional[Union[torch.FloatTensor, list]] = None,\n",
    "            head_start_step: Optional[int] = None,\n",
    "            height: Optional[int] = None,\n",
    "            width: Optional[int] = None,\n",
    "            num_inference_steps: int = 50,\n",
    "            guidance_scale: float = 7.5,\n",
    "            negative_prompt: Optional[Union[str, List[str]]] = None,\n",
    "            num_images_per_prompt: Optional[int] = 1,\n",
    "            eta: float = 0.0,\n",
    "            generator: Optional[torch.Generator] = None,\n",
    "            latents: Optional[torch.FloatTensor] = None,\n",
    "            output_type: Optional[str] = \"pil\",\n",
    "            return_dict: bool = True,\n",
    "            callback: Optional[Callable[[int, int, torch.FloatTensor], None]] = None,\n",
    "            callback_steps: Optional[int] = 1,\n",
    "    ):\n",
    "        r\"\"\"\n",
    "        Function invoked when calling the pipeline for generation.\n",
    "\n",
    "        Args:\n",
    "            prompt (`str` or `List[str]`):\n",
    "                The prompt or prompts to guide the image generation.\n",
    "            height (`int`, *optional*, defaults to self.unet.config.sample_size * self.vae_scale_factor):\n",
    "                The height in pixels of the generated image.\n",
    "            width (`int`, *optional*, defaults to self.unet.config.sample_size * self.vae_scale_factor):\n",
    "                The width in pixels of the generated image.\n",
    "            num_inference_steps (`int`, *optional*, defaults to 50):\n",
    "                The number of denoising steps. More denoising steps usually lead to a higher quality image at the\n",
    "                expense of slower inference.\n",
    "            guidance_scale (`float`, *optional*, defaults to 7.5):\n",
    "                Guidance scale as defined in [Classifier-Free Diffusion Guidance](https://arxiv.org/abs/2207.12598).\n",
    "                `guidance_scale` is defined as `w` of equation 2. of [Imagen\n",
    "                Paper](https://arxiv.org/pdf/2205.11487.pdf). Guidance scale is enabled by setting `guidance_scale >\n",
    "                1`. Higher guidance scale encourages to generate images that are closely linked to the text `prompt`,\n",
    "                usually at the expense of lower image quality.\n",
    "            negative_prompt (`str` or `List[str]`, *optional*):\n",
    "                The prompt or prompts not to guide the image generation. Ignored when not using guidance (i.e., ignored\n",
    "                if `guidance_scale` is less than `1`).\n",
    "            num_images_per_prompt (`int`, *optional*, defaults to 1):\n",
    "                The number of images to generate per prompt.\n",
    "            eta (`float`, *optional*, defaults to 0.0):\n",
    "                Corresponds to parameter eta (η) in the DDIM paper: https://arxiv.org/abs/2010.02502. Only applies to\n",
    "                [`schedulers.DDIMScheduler`], will be ignored for others.\n",
    "            generator (`torch.Generator`, *optional*):\n",
    "                A [torch generator](https://pytorch.org/docs/stable/generated/torch.Generator.html) to make generation\n",
    "                deterministic.\n",
    "            latents (`torch.FloatTensor`, *optional*):\n",
    "                Pre-generated noisy latents, sampled from a Gaussian distribution, to be used as inputs for image\n",
    "                generation. Can be used to tweak the same generation with different prompts. If not provided, a latents\n",
    "                tensor will ge generated by sampling using the supplied random `generator`.\n",
    "            output_type (`str`, *optional*, defaults to `\"pil\"`):\n",
    "                The output format of the generate image. Choose between\n",
    "                [PIL](https://pillow.readthedocs.io/en/stable/): `PIL.Image.Image` or `np.array`.\n",
    "            return_dict (`bool`, *optional*, defaults to `True`):\n",
    "                Whether or not to return a [`~pipelines.stable_diffusion.StableDiffusionPipelineOutput`] instead of a\n",
    "                plain tuple.\n",
    "            callback (`Callable`, *optional*):\n",
    "                A function that will be called every `callback_steps` steps during inference. The function will be\n",
    "                called with the following arguments: `callback(step: int, timestep: int, latents: torch.FloatTensor)`.\n",
    "            callback_steps (`int`, *optional*, defaults to 1):\n",
    "                The frequency at which the `callback` function will be called. If not specified, the callback will be\n",
    "                called at every step.\n",
    "\n",
    "        Returns:\n",
    "            [`~pipelines.stable_diffusion.StableDiffusionPipelineOutput`] or `tuple`:\n",
    "            [`~pipelines.stable_diffusion.StableDiffusionPipelineOutput`] if `return_dict` is True, otherwise a `tuple.\n",
    "            When returning a tuple, the first element is a list with the generated images, and the second element is a\n",
    "            list of `bool`s denoting whether the corresponding generated image likely represents \"not-safe-for-work\"\n",
    "            (nsfw) content, according to the `safety_checker`.\n",
    "        \"\"\"\n",
    "        # import pdb; pdb.set_trace()\n",
    "        # 0. Default height and width to unet\n",
    "        height = height or self.unet.config.sample_size * self.vae_scale_factor\n",
    "        width = width or self.unet.config.sample_size * self.vae_scale_factor\n",
    "\n",
    "        # 1. Check inputs. Raise error if not correct\n",
    "        self.check_inputs(prompt, height, width, callback_steps)\n",
    "\n",
    "        # 2. Define call parameters\n",
    "        batch_size = 1 if isinstance(prompt, str) else len(prompt)\n",
    "        device = self._execution_device\n",
    "        # here `guidance_scale` is defined analog to the guidance weight `w` of equation (2)\n",
    "        # of the Imagen paper: https://arxiv.org/pdf/2205.11487.pdf . `guidance_scale = 1`\n",
    "        # corresponds to doing no classifier free guidance.\n",
    "        do_classifier_free_guidance = guidance_scale > 1.0\n",
    "\n",
    "        # 3. Encode input prompt\n",
    "        text_embeddings = self._encode_prompt(\n",
    "            prompt, device, num_images_per_prompt, do_classifier_free_guidance, negative_prompt\n",
    "        )\n",
    "\n",
    "        if prompt2 is not None:\n",
    "            text_embeddings2 = self._encode_prompt(\n",
    "                prompt2, device, num_images_per_prompt, do_classifier_free_guidance, negative_prompt\n",
    "            )\n",
    "\n",
    "        # 4. Prepare timesteps\n",
    "        self.scheduler.set_timesteps(num_inference_steps, device=device)\n",
    "        timesteps = self.scheduler.timesteps\n",
    "        # print(timesteps)\n",
    "\n",
    "        # 5. Prepare latent variables\n",
    "        if head_start_latents is None:\n",
    "            num_channels_latents = self.unet.in_channels\n",
    "            latents = self.prepare_latents(\n",
    "                batch_size * num_images_per_prompt,\n",
    "                num_channels_latents,\n",
    "                height,\n",
    "                width,\n",
    "                text_embeddings.dtype,\n",
    "                device,\n",
    "                generator,\n",
    "                latents,\n",
    "            )\n",
    "        else:\n",
    "            if type(head_start_latents) == list:\n",
    "                latents = head_start_latents[-1]\n",
    "                assert len(head_start_latents) == self.scheduler.config.solver_order\n",
    "\n",
    "            else:\n",
    "                latents = head_start_latents  # if there is a head start\n",
    "        # 6. Prepare extra step kwargs. TODO: Logic should ideally just be moved out of the pipeline\n",
    "        extra_step_kwargs = self.prepare_extra_step_kwargs(generator, eta)\n",
    "\n",
    "        # 7. Denoising loop\n",
    "        num_warmup_steps = len(timesteps) - num_inference_steps * self.scheduler.order\n",
    "        # with self.progress_bar(total=num_inference_steps) as progress_bar:\n",
    "        for i, t in enumerate(timesteps):\n",
    "            # print((i, t))\n",
    "            if not head_start_step or i >= head_start_step:  # if there is no head start or we reached the hs step\n",
    "                # expand the latents if we are doing classifier free guidance\n",
    "                # print(latents.shape)\n",
    "                latent_model_input = torch.cat([latents] * 2) if do_classifier_free_guidance else latents\n",
    "                latent_model_input = self.scheduler.scale_model_input(latent_model_input, t)\n",
    "\n",
    "                # predict the noise residual\n",
    "                if prompt1_steps is None or i < prompt1_steps:\n",
    "                    noise_pred = self.unet(latent_model_input, t, encoder_hidden_states=text_embeddings).sample\n",
    "                else:\n",
    "                    # print(f'i = {i}, atteding to prompt2')\n",
    "                    noise_pred = self.unet(latent_model_input, t, encoder_hidden_states=text_embeddings2).sample\n",
    "\n",
    "                # perform guidance\n",
    "                if do_classifier_free_guidance:\n",
    "                    noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
    "                    noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
    "\n",
    "                    # compute the previous noisy sample x_t -> x_t-1\n",
    "                    latents = self.scheduler.step(noise_pred, t, latents, **extra_step_kwargs).prev_sample\n",
    "\n",
    "            # call the callback, if provided\n",
    "            if (i + 1) > num_warmup_steps and (i + 1) % self.scheduler.order == 0:\n",
    "                # progress_bar.update()\n",
    "                if callback is not None and i % callback_steps == 0:\n",
    "                    callback(i, t, latents)\n",
    "\n",
    "        # 8. Post-processing\n",
    "        image = self.decode_latents(latents)\n",
    "\n",
    "        # 9. Run safety checker\n",
    "        has_nsfw_concept = False\n",
    "        # image, has_nsfw_concept = self.run_safety_checker(image, device, text_embeddings.dtype)\n",
    "\n",
    "        # 10. Convert to PIL\n",
    "        if output_type == \"pil\":\n",
    "            image = self.numpy_to_pil(image)\n",
    "\n",
    "        if not return_dict:\n",
    "            return (image, has_nsfw_concept)\n",
    "\n",
    "        return StableDiffusionPipelineOutput(images=image, nsfw_content_detected=has_nsfw_concept)\n",
    "\n",
    "#TODO cite this too\n",
    "class DiffWMAttacker:\n",
    "    def __init__(self, pipe, noise_step=60, captions={}):\n",
    "        self.pipe = pipe\n",
    "        self.noise_step = noise_step\n",
    "        self.captions = captions\n",
    "        print(\n",
    "            f\"Diffuse attack initialized with noise step {self.noise_step} and use prompt {len(self.captions)}\"\n",
    "        )\n",
    "\n",
    "    def attack(self, image, return_latents=False, return_dist=False):\n",
    "        with torch.no_grad():\n",
    "            generator = torch.Generator(DEVICE).manual_seed(1024)\n",
    "            latents_buf = []\n",
    "            prompts_buf = []\n",
    "            outs_buf = []\n",
    "            timestep = torch.tensor(\n",
    "                [self.noise_step], dtype=torch.long, device=DEVICE\n",
    "            )\n",
    "            ret_latents = []\n",
    "\n",
    "            def batched_attack(latents_buf, prompts_buf, outs_buf):\n",
    "                latents = torch.cat(latents_buf, dim=0)\n",
    "                images = self.pipe(\n",
    "                    prompts_buf,\n",
    "                    head_start_latents=latents,\n",
    "                    head_start_step=50 - max(self.noise_step // 20, 1),\n",
    "                    guidance_scale=7.5,\n",
    "                    generator=generator,\n",
    "                )\n",
    "                images = images[0]\n",
    "                for img, out in zip(images, outs_buf):\n",
    "                    return img\n",
    "\n",
    "            img = np.asarray(image) / 255\n",
    "            img = (img - 0.5) * 2\n",
    "            img = torch.tensor(img).permute(2, 0, 1).unsqueeze(0)\n",
    "            latents = self.pipe.vae.encode(\n",
    "                img.to(device=DEVICE, dtype=torch.float16) #remove when running on cuda?\n",
    "            ).latent_dist\n",
    "            latents = latents.sample(generator) * self.pipe.vae.config.scaling_factor\n",
    "            noise = torch.randn(\n",
    "                [1, 4, img.shape[-2] // 8, img.shape[-1] // 8], device=DEVICE\n",
    "            )\n",
    "\n",
    "            latents = self.pipe.scheduler.add_noise(latents, noise, timestep).type(\n",
    "                torch.half\n",
    "            )\n",
    "            latents_buf.append(latents)\n",
    "            outs_buf.append(\"\")\n",
    "            prompts_buf.append(\"\")\n",
    "\n",
    "            img = batched_attack(latents_buf, prompts_buf, outs_buf)\n",
    "            return img\n",
    "\n",
    "\n",
    "def remove_watermark(attack_method, attacker, image, strength, model):\n",
    "    # create attacker\n",
    "    # print(f\"Creating attacker {attack_method}...\")\n",
    "    if attack_method == \"regen_diffusion\":\n",
    "        img = attacker.attack(image)\n",
    "        return img\n",
    "    else:\n",
    "        raise Exception(f\"Unknown attacking method: {attack_method}!\")\n",
    "\n",
    "    \n",
    "\n",
    "def rinse_nxDiff(image, attacker, strength=10, n=2, model=\"stabilityai/stable-diffusion-2-1\"):\n",
    "    attack = \"regen_diffusion\"\n",
    "    for i in range(n):\n",
    "        image = remove_watermark(attack, attacker, image, strength, model)\n",
    "    return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘../attacked_stable_sig’: File exists\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pipeline components...:  29%|██▊       | 2/7 [00:00<00:00,  9.31it/s]An error occurred while trying to fetch /home/tomg/.cache/huggingface/hub/models--CompVis--stable-diffusion-v1-4/snapshots/2880f2ca379f41b0226444936bb7a6766a227587/vae: Error no file named diffusion_pytorch_model.safetensors found in directory /home/tomg/.cache/huggingface/hub/models--CompVis--stable-diffusion-v1-4/snapshots/2880f2ca379f41b0226444936bb7a6766a227587/vae.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n",
      "Loading pipeline components...:  57%|█████▋    | 4/7 [00:00<00:00, 12.63it/s]An error occurred while trying to fetch /home/tomg/.cache/huggingface/hub/models--CompVis--stable-diffusion-v1-4/snapshots/2880f2ca379f41b0226444936bb7a6766a227587/unet: Error no file named diffusion_pytorch_model.safetensors found in directory /home/tomg/.cache/huggingface/hub/models--CompVis--stable-diffusion-v1-4/snapshots/2880f2ca379f41b0226444936bb7a6766a227587/unet.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n",
      "Loading pipeline components...: 100%|██████████| 7/7 [00:01<00:00,  5.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diffuse attack initialized with noise step 20 and use prompt 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5000 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'progress_bar' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 25\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# print(img_name)\u001b[39;00m\n\u001b[1;32m     24\u001b[0m img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(img_path)\n\u001b[0;32m---> 25\u001b[0m img \u001b[38;5;241m=\u001b[39m \u001b[43mrinse_nxDiff\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattacker\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrength\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrength\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_rinse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m img\u001b[38;5;241m.\u001b[39msave(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(attacked_imgs_folder, img_name))\n",
      "Cell \u001b[0;32mIn[11], line 259\u001b[0m, in \u001b[0;36mrinse_nxDiff\u001b[0;34m(image, attacker, strength, n, model)\u001b[0m\n\u001b[1;32m    257\u001b[0m attack \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mregen_diffusion\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n):\n\u001b[0;32m--> 259\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[43mremove_watermark\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattack\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattacker\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrength\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m image\n",
      "Cell \u001b[0;32mIn[11], line 249\u001b[0m, in \u001b[0;36mremove_watermark\u001b[0;34m(attack_method, attacker, image, strength, model)\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mremove_watermark\u001b[39m(attack_method, attacker, image, strength, model):\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;66;03m# create attacker\u001b[39;00m\n\u001b[1;32m    247\u001b[0m     \u001b[38;5;66;03m# print(f\"Creating attacker {attack_method}...\")\u001b[39;00m\n\u001b[1;32m    248\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attack_method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mregen_diffusion\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 249\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mattacker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    250\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m img\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn[11], line 241\u001b[0m, in \u001b[0;36mDiffWMAttacker.attack\u001b[0;34m(self, image, return_latents, return_dist)\u001b[0m\n\u001b[1;32m    238\u001b[0m outs_buf\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    239\u001b[0m prompts_buf\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 241\u001b[0m img \u001b[38;5;241m=\u001b[39m \u001b[43mbatched_attack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlatents_buf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompts_buf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mouts_buf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "Cell \u001b[0;32mIn[11], line 212\u001b[0m, in \u001b[0;36mDiffWMAttacker.attack.<locals>.batched_attack\u001b[0;34m(latents_buf, prompts_buf, outs_buf)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbatched_attack\u001b[39m(latents_buf, prompts_buf, outs_buf):\n\u001b[1;32m    211\u001b[0m     latents \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(latents_buf, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m--> 212\u001b[0m     images \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpipe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompts_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_start_latents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlatents\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_start_step\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mmax\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnoise_step\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[43m        \u001b[49m\u001b[43mguidance_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m7.5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgenerator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    219\u001b[0m     images \u001b[38;5;241m=\u001b[39m images[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m img, out \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(images, outs_buf):\n",
      "File \u001b[0;32m~/src/IDL_WAR/venv/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[11], line 169\u001b[0m, in \u001b[0;36mReSDPipeline.__call__\u001b[0;34m(self, prompt, prompt1_steps, prompt2, head_start_latents, head_start_step, height, width, num_inference_steps, guidance_scale, negative_prompt, num_images_per_prompt, eta, generator, latents, output_type, return_dict, callback, callback_steps)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;66;03m# call the callback, if provided\u001b[39;00m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m>\u001b[39m num_warmup_steps \u001b[38;5;129;01mand\u001b[39;00m (i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscheduler\u001b[38;5;241m.\u001b[39morder \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 169\u001b[0m     \u001b[43mprogress_bar\u001b[49m\u001b[38;5;241m.\u001b[39mupdate()\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m callback \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m i \u001b[38;5;241m%\u001b[39m callback_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    171\u001b[0m         callback(i, t, latents)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'progress_bar' is not defined"
     ]
    }
   ],
   "source": [
    "# Model selection: [\"CompVis/stable-diffusion-v1-4\",\"stabilityai/stable-diffusion-2-1\"]\n",
    "watermarked_imgs_folder = \"../stable_sig/\"\n",
    "!mkdir ../attacked_stable_sig\n",
    "attacked_imgs_folder = \"../attacked_stable_sig/\"\n",
    "\n",
    "paper_rinse2xdiff_config = (\"CompVis/stable-diffusion-v1-4\", 2, 20)\n",
    "paper_rinse4xdiff_config = (\"CompVis/stable-diffusion-v1-4\", 4, 10)\n",
    "dj_rinse1xdiff_config = (\"stabilityai/stable-diffusion-2-1\", 1, 10)\n",
    "\n",
    "config = paper_rinse2xdiff_config\n",
    "\n",
    "model, n_rinse, strength = config\n",
    "\n",
    "pipe = ReSDPipeline.from_pretrained(\n",
    "    model, torch_dtype=torch.float16, revision=\"fp16\"\n",
    ")\n",
    "# pipe.set_progress_bar_config(disable=True)\n",
    "pipe.to(DEVICE)\n",
    "attacker = DiffWMAttacker(pipe, noise_step=strength, captions={})\n",
    "# Loop over images in watermarked_imgs_folder\n",
    "for img_name in tqdm(os.listdir(watermarked_imgs_folder)):\n",
    "    img_path = os.path.join(watermarked_imgs_folder, img_name)\n",
    "    # print(img_name)\n",
    "    img = Image.open(img_path)\n",
    "    img = rinse_nxDiff(img, attacker, strength=strength, n=n_rinse, model=model)\n",
    "    img.save(os.path.join(attacked_imgs_folder, img_name))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
